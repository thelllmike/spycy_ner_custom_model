{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "feb5c8c6-6d86-4ce7-8cf2-12331f3474a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing JSON: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  3.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ… Saved to: cv_skills_dataset.spacy\n",
      "ðŸ“Š Total: 15, âœ… Success: 15, âŒ Failed: 0, âš ï¸ Skipped spans: 28\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "import os\n",
    "import json\n",
    "from spacy.tokens import DocBin\n",
    "from tqdm import tqdm\n",
    "\n",
    "nlp = spacy.blank(\"en\")\n",
    "doc_bin = DocBin()\n",
    "\n",
    "DATASET_PATH = \"dataset\"\n",
    "\n",
    "def spans_overlap(span1, span2):\n",
    "    return span1.start < span2.end and span2.start < span1.end\n",
    "\n",
    "total_docs = 0\n",
    "successful_docs = 0\n",
    "failed_docs = 0\n",
    "skipped_spans = 0\n",
    "\n",
    "for filename in tqdm(os.listdir(DATASET_PATH), desc=\"Processing JSON\"):\n",
    "    if not filename.endswith(\".json\"):\n",
    "        continue\n",
    "\n",
    "    path = os.path.join(DATASET_PATH, filename)\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        data_list = json.load(f)\n",
    "\n",
    "    for item in data_list:\n",
    "        total_docs += 1\n",
    "        try:\n",
    "            text = item[\"data\"][\"text\"]\n",
    "            annotations = item.get(\"annotations\")\n",
    "        except Exception as e:\n",
    "            failed_docs += 1\n",
    "            print(f\"âŒ Failed to extract text or annotations from item: {e}\")\n",
    "            continue\n",
    "\n",
    "        if not text or not annotations:\n",
    "            failed_docs += 1\n",
    "            continue\n",
    "\n",
    "        results = annotations[0].get(\"result\", [])\n",
    "        doc = nlp.make_doc(text)\n",
    "        ents = []\n",
    "\n",
    "        for result in results:\n",
    "            value = result.get(\"value\", {})\n",
    "            start = value.get(\"start\")\n",
    "            end = value.get(\"end\")\n",
    "            label_list = value.get(\"labels\", [])\n",
    "\n",
    "            if start is None or end is None or not label_list:\n",
    "                continue\n",
    "\n",
    "            label = label_list[0].strip()\n",
    "            span = doc.char_span(start, end, label=label, alignment_mode=\"contract\")\n",
    "\n",
    "            if span is None:\n",
    "                skipped_spans += 1\n",
    "                continue\n",
    "\n",
    "            if any(spans_overlap(span, existing) for existing in ents):\n",
    "                skipped_spans += 1\n",
    "                continue\n",
    "\n",
    "            ents.append(span)\n",
    "\n",
    "        try:\n",
    "            doc.ents = ents\n",
    "            doc_bin.add(doc)\n",
    "            successful_docs += 1\n",
    "        except Exception as e:\n",
    "            failed_docs += 1\n",
    "            print(f\"âŒ Error in item ID {item.get('id')}: {e}\")\n",
    "\n",
    "# Save output\n",
    "output_file = \"cv_skills_dataset.spacy\"\n",
    "doc_bin.to_disk(output_file)\n",
    "\n",
    "print(f\"\\nâœ… Saved to: {output_file}\")\n",
    "print(f\"ðŸ“Š Total: {total_docs}, âœ… Success: {successful_docs}, âŒ Failed: {failed_docs}, âš ï¸ Skipped spans: {skipped_spans}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "88460f4d-d376-4567-aea0-3e25fc5183d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Done: train.spacy and dev.spacy saved.\n"
     ]
    }
   ],
   "source": [
    "from spacy.tokens import DocBin\n",
    "import random\n",
    "\n",
    "db = DocBin().from_disk(\"cv_skills_dataset.spacy\")\n",
    "docs = list(db.get_docs(spacy.blank(\"en\").vocab))\n",
    "\n",
    "random.shuffle(docs)\n",
    "split = int(0.8 * len(docs))\n",
    "\n",
    "DocBin(docs=docs[:split]).to_disk(\"train.spacy\")\n",
    "DocBin(docs=docs[split:]).to_disk(\"dev.spacy\")\n",
    "print(\"âœ… Done: train.spacy and dev.spacy saved.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eea169a5-caf3-46aa-9ae9-96164797f9a1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
